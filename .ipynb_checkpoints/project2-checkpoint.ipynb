{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Utility methods\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "# do PCA to reduce dimensionality, advisable for RVM and GPR to reduce training time\n",
    "def reduce_dimensions(data):\n",
    "    pca = decomposition.PCA(n_components = 7)\n",
    "\n",
    "    X = pca.fit_transform(data)\n",
    "    return X\n",
    "\n",
    "# reads model from pickled object file\n",
    "def readObj(name):\n",
    "    with open(cfg.path + name, 'rb') as input:\n",
    "        clf = pickle.load(input)\n",
    "\n",
    "    return clf\n",
    "\n",
    "# writes model to a pickled object file\n",
    "def writeObj(name, obj):\n",
    "    with open(cfg.path + name, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#zips input-output data into list of tuples\n",
    "def zipData(X, Y):\n",
    "    return zip(X, Y)\n",
    "\n",
    "#unzips the data and converts output [1, -1...] to classLabels\n",
    "def unzipData(X):\n",
    "    x, y = zip(*X)\n",
    "\n",
    "    x = np.array(list(x))\n",
    "    y = np.array([np.where(output == 1)[0][0] for output in list(y)])\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "pretty prints confusion matrix and returns confusion matrix and accuracy score\n",
    "\n",
    "@param   Y                   predicted labels\n",
    "\n",
    "@param   ClassLabels         actual / true labels\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def MyConfusionMatrix(Y, ClassNames):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(Y, ClassNames)\n",
    "    conf_matrix = np.around(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis], decimals = 2)\n",
    "    \n",
    "    accuracy = accuracy_score(Y, ClassNames)\n",
    "\n",
    "    ClassLabels = range(len(conf_matrix))\n",
    "\n",
    "    columns = tuple(ClassLabels)\n",
    "    rows = tuple(ClassLabels)\n",
    "\n",
    "    df = pd.DataFrame(data=conf_matrix, columns=ClassLabels)\n",
    "\n",
    "    print \"\\nconfusion matrix: \\n\"\n",
    "    print df\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"accuracy: \", accuracy\n",
    "\n",
    "    return conf_matrix, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "5 is the garbage class\n",
    "\n",
    "overall confusion matrix (SVM):\n",
    "\n",
    "    0     1     2     3     4     5\n",
    "0  0.99  0.00  0.01  0.00  0.00  0.0\n",
    "1  0.00  0.98  0.00  0.02  0.00  0.0\n",
    "2  0.00  0.00  0.99  0.00  0.01  0.0\n",
    "3  0.00  0.02  0.00  0.97  0.01  0.0\n",
    "4  0.00  0.00  0.01  0.01  0.98  0.0\n",
    "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs One Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All Pairs Training\n",
    "\n",
    "we rolled out our own OneVsOneClassifier which takes in a binary classifier and trains nC2 classifiers\n",
    "this was done as the default sklearn OneVsOne doesn't support probability estimates for the classes\n",
    "which would be useful for determining garbage class inputs\n",
    "\n",
    "\"\"\"\n",
    "class OneVsOne:\n",
    "    def __init__(self, model):\n",
    "        self.model_ = model\n",
    "\n",
    "    def fact(self, n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "\n",
    "        return n*self.fact(n-1)\n",
    "\n",
    "    def nCr(self, n, r):\n",
    "        return self.fact(n)/(self.fact(n-r)*self.fact(r))\n",
    "\n",
    "    def getModel(self):\n",
    "        if self.model_ == 'gaussian':\n",
    "            return GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "        elif self.model_ == 'svm':\n",
    "            return SVC(C=100, kernel='linear', probability=True)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.Nclasses_     = len(np.unique(Y))\n",
    "        self.Nclassifiers_ = self.nCr(self.Nclasses_, 2)\n",
    "\n",
    "        Nclasses = self.Nclasses_\n",
    "        Nclassifiers = self.Nclassifiers_\n",
    "\n",
    "        dataparts = [None]*Nclasses\n",
    "        classifiers = [[None]*Nclasses]*Nclasses\n",
    "\n",
    "        print classifiers\n",
    "\n",
    "        for i in range(Nclasses):\n",
    "            dataparts[i] = np.where(Y == i)[0]\n",
    "\n",
    "        for i in range(Nclasses):\n",
    "            for j in range(i+1, Nclasses):\n",
    "                print \"training classifier: \", i, \" \",j\n",
    "\n",
    "                xi = X[dataparts[i]]\n",
    "                xj = X[dataparts[j]]\n",
    "\n",
    "                yi = [0]*len(xi)\n",
    "                yj = [1]*len(xj)\n",
    "\n",
    "                x = np.vstack([xi, xj])\n",
    "                y = np.hstack([yi, yj])\n",
    "\n",
    "                clf = self.getModel()\n",
    "\n",
    "                print \"clf fitting\"\n",
    "                clf.fit(x, y)\n",
    "                print \"clf fitting done\"\n",
    "\n",
    "                classifiers[i][j] = clf\n",
    "\n",
    "        self.classifiers = classifiers\n",
    "        #print classifiers\n",
    "\n",
    "    def predict(self, X):\n",
    "        Nclasses = self.Nclasses_\n",
    "        Nclassifiers = self.Nclassifiers_\n",
    "\n",
    "        classifiers = self.classifiers\n",
    "\n",
    "        Y = []\n",
    "\n",
    "        for x in X:\n",
    "            probabilities = [0]*Nclasses\n",
    "\n",
    "            for i in range(Nclasses):\n",
    "                for j in range(i+1, Nclasses):\n",
    "                    clf = classifiers[i][j]\n",
    "\n",
    "                    probabilities[i] += clf.predict_proba([x])[0][0]\n",
    "                    probabilities[j] += clf.predict_proba([x])[0][1]\n",
    "\n",
    "            probabilities = [p / 10.0 for p in probabilities]\n",
    "\n",
    "            Y.append(probabilities.index(max(probabilities)))\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "example usage of OneVsOne : \n",
    "\n",
    "```\n",
    "clf = OneVsOne('svm')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "clf.predict(X)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM (Support Vector Machine):\n",
    "\n",
    "performs grid search to compute optimal hyper-parameters\n",
    "uses those hyper-parameters for the estimator, fits it on the training data\n",
    "\n",
    "returns trained model and writes it to file for transfer learning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def SVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params):\n",
    "\n",
    "    hyper_param_grid = [\n",
    "        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "    ]\n",
    "\n",
    "    train = params['train']\n",
    "\n",
    "    if train:\n",
    "        estimator = GridSearchCV(SVC(decision_function_shape='ovo', probability=True), hyper_param_grid, cv=3, scoring='precision_macro')\n",
    "\n",
    "        print \"SVM: executing grid search to find optimal hyper-parameters\"\n",
    "\n",
    "        estimator.fit(X_hyper, Y_hyper)\n",
    "        clf = estimator.best_estimator_\n",
    "        \n",
    "        print \"found best hyperparameters:\"\n",
    "\n",
    "        print estimator.best_params_\n",
    "        print \"training the estimator\"\n",
    "\n",
    "        clf.fit(X_train, Y_train)\n",
    "        print(\"number of support vectors:\", len(clf.support_))\n",
    "        \n",
    "        writeObj('svm_model.pkl', clf)\n",
    "\n",
    "        Y_pred = clf.predict(X_validate)\n",
    "        return Y_pred, clf\n",
    "\n",
    "    else:\n",
    "        clf = readObj('svm_model.pkl')\n",
    "        print(\"number of support vectors:\", len(clf.support_))\n",
    "        \n",
    "        Y_pred = clf.predict(X_validate)\n",
    "        return Y_pred, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RVM (Relevance Vector Machine):\n",
    "\n",
    "uses PCA to reduce dimensionality as RVM training takes a long time\n",
    "also uses a subset of training data to save time\n",
    "\n",
    "\n",
    "returns trained model and writes it to file for transfer learning\n",
    "\n",
    "\"\"\"\n",
    "from skrvm import RVC\n",
    "\n",
    "def RVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params):\n",
    "    clf = RVC(n_iter=100, tol=0.1)\n",
    "    start = time.clock()\n",
    "\n",
    "    X_train_reduced = X_train\n",
    "    X_validate_reduced = X_validate\n",
    "\n",
    "    train_size = params['train_size']\n",
    "    test_size  = params['test_size']\n",
    "    train      = params['train']\n",
    "\n",
    "    if train:\n",
    "        clf.fit(X_train_reduced[:train_size, :], Y_train[:train_size])\n",
    "        writeObj('rvm_model.pkl', clf)\n",
    "\n",
    "        Y_pred = clf.predict(X_validate_reduced[:test_size])\n",
    "        return Y_pred, clf\n",
    "    else:\n",
    "        clf = readObj('rvm_model.pkl')\n",
    "        Y_pred = clf.predict(X_validate_reduced[:test_size])\n",
    "        return Y_pred, clf\n",
    "\n",
    "    print \"training took \", time.clock() - start, \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPR (Gaussian Process Regressor):\n",
    "\n",
    "uses PCA to reduce dimensionality as training takes a long time\n",
    "also uses a subset of training data to save time\n",
    "\n",
    "returns trained model and writes it to file for transfer learning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pickle\n",
    "\n",
    "def GPR(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params):\n",
    "    print \"GPR training :\"\n",
    "\n",
    "    X_train_reduced = X_train\n",
    "    X_validate_reduced = X_validate\n",
    "\n",
    "    train_size = params['train_size']\n",
    "    test_size  = params['test_size']\n",
    "    train      = params['train']\n",
    "\n",
    "    if train:\n",
    "        start = time.clock()\n",
    "        kernel_rbf = 1.0 * RBF()\n",
    "\n",
    "        clf = GaussianProcessClassifier(kernel=kernel_rbf, multi_class='one_vs_rest')\n",
    "        clf.fit(X_train_reduced[:train_size, :], Y_train[:train_size])\n",
    "\n",
    "        writeObj('gaussian_model.pkl', clf)\n",
    "        print \"training took \", time.clock() - start, \" s\"\n",
    "\n",
    "        Y_pred = clf.predict(X_validate_reduced[:test_size])\n",
    "        return Y_pred, clf\n",
    "    else:\n",
    "        clf = readObj('gaussian_model.pkl')\n",
    "        Y_pred = clf.predict(X_validate_reduced[:test_size])\n",
    "\n",
    "        return Y_pred, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def MyTrainClassifier(XEstimate, XValidate, Parameters):\n",
    "\n",
    "    X_train, Y_train = unzipData(XEstimate)\n",
    "\n",
    "    # sampling a small amount of training data for finding optimal hyper-parameters\n",
    "    X_hyper = X_train[:cfg.hyper_train_size, :]\n",
    "    Y_hyper = Y_train[:cfg.hyper_train_size]\n",
    "\n",
    "    X_validate, Y_validate = unzipData(XValidate)\n",
    "\n",
    "    train = Parameters['training_mode']\n",
    "    params = { 'train' : train, 'train_size' : cfg.train_size, 'test_size' : cfg.test_size }\n",
    "\n",
    "    if Parameters['algorithm'] == 'SVM':\n",
    "\n",
    "        Y_predict, model = SVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params)\n",
    "\n",
    "    elif Parameters['algorithm'] == 'RVM':\n",
    "\n",
    "        Y_predict, model = RVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params)\n",
    "\n",
    "    elif Parameters['algorithm'] == 'GPR':\n",
    "\n",
    "        Y_predict, model = GPR(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate, params)\n",
    "\n",
    "    return Y_predict, {'model' : model, 'algorithm' : Parameters['algorithm'], 'test_size' : params['test_size']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "takes in XTest ( which is a zipped form of input and output data tuples ) and a trained model.\n",
    "evaluates the performance of the model\n",
    "\n",
    "\"\"\"\n",
    "def TestMyClassifier(XTest, EstParameters):\n",
    "    model = EstParameters['model']\n",
    "\n",
    "    Xactual, _ = unzipData(XTest)\n",
    "    Ytest = model.predict(Xactual)\n",
    "\n",
    "    return Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "performs K-fold cross validation and selects the best model to prevent overfitting\n",
    "returns the array of confusion matrices and estimated parameter models for every fold\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "from random import shuffle\n",
    "\n",
    "def MyCrossValidate(XTrain, Nf):\n",
    "    shuffle(XTrain)\n",
    "    kf = KFold(n_splits = Nf)\n",
    "\n",
    "    j = 1\n",
    "\n",
    "    EstParameters = []\n",
    "    EstConfMatrices = []\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(XTrain):\n",
    "        En = [XTrain[i] for i in train_index]\n",
    "        Vn = [XTrain[i] for i in test_index]\n",
    "\n",
    "        print \"\\nfold {} in progress:\\n\".format(j)\n",
    "\n",
    "        Y_predicted, EstParameter = MyTrainClassifier(En, Vn, {'algorithm' : cfg.algorithm, 'training_mode' : cfg.train})\n",
    "\n",
    "        _, Y_validate = unzipData(Vn)\n",
    "\n",
    "        algorithm = EstParameter['algorithm']\n",
    "\n",
    "        if algorithm == 'GPR' or algorithm ==  'RVM':\n",
    "            Y_validate = Y_validate[:EstParameter['test_size']]\n",
    "\n",
    "        Cn, acc = MyConfusionMatrix(Y_predicted, Y_validate)\n",
    "\n",
    "        EstParameter['accuracy'] = acc\n",
    "\n",
    "        EstConfMatrices.append(Cn)\n",
    "        EstParameters.append(EstParameter)\n",
    "\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    print \"\"\n",
    "\n",
    "    best_model_idx = accuracies.index(max(accuracies))\n",
    "    best_model = EstParameters[best_model_idx]['model']\n",
    "\n",
    "    X, Y = unzipData(XTrain)\n",
    "\n",
    "    algorithm = EstParameters[best_model_idx]['algorithm']\n",
    "    YTrain = best_model.predict(X)\n",
    "\n",
    "    print \"overall confusion matrix :\"\n",
    "\n",
    "    ConfMatrix, acc = MyConfusionMatrix(YTrain, Y)\n",
    "\n",
    "    return YTrain, EstParameters, EstConfMatrices, ConfMatrix, best_model_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1 in progress:\n",
      "\n",
      "('number of support vectors:', 1986)\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.99  0.00  0.00  0.00  0.00  0.0\n",
      "1  0.00  0.98  0.00  0.02  0.00  0.0\n",
      "2  0.00  0.00  0.99  0.00  0.01  0.0\n",
      "3  0.00  0.02  0.00  0.98  0.01  0.0\n",
      "4  0.00  0.00  0.00  0.00  0.99  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.987166666667\n",
      "\n",
      "fold 2 in progress:\n",
      "\n",
      "('number of support vectors:', 1986)\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.98  0.00  0.01  0.00  0.00  0.0\n",
      "1  0.00  0.97  0.00  0.03  0.00  0.0\n",
      "2  0.00  0.00  0.99  0.00  0.00  0.0\n",
      "3  0.00  0.02  0.00  0.97  0.01  0.0\n",
      "4  0.00  0.00  0.01  0.01  0.99  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.983166666667\n",
      "\n",
      "fold 3 in progress:\n",
      "\n",
      "('number of support vectors:', 1986)\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.98  0.00  0.01  0.01  0.00  0.0\n",
      "1  0.00  0.98  0.00  0.01  0.00  0.0\n",
      "2  0.00  0.00  0.99  0.00  0.00  0.0\n",
      "3  0.00  0.02  0.00  0.98  0.00  0.0\n",
      "4  0.00  0.00  0.01  0.01  0.98  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.985833333333\n",
      "\n",
      "fold 4 in progress:\n",
      "\n",
      "('number of support vectors:', 1986)\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.99  0.00  0.00  0.00  0.00  0.0\n",
      "1  0.00  0.97  0.00  0.02  0.00  0.0\n",
      "2  0.00  0.00  0.99  0.00  0.01  0.0\n",
      "3  0.00  0.02  0.00  0.97  0.00  0.0\n",
      "4  0.00  0.00  0.01  0.00  0.98  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.983833333333\n",
      "\n",
      "fold 5 in progress:\n",
      "\n",
      "('number of support vectors:', 1986)\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.99  0.00  0.01  0.00  0.00  0.0\n",
      "1  0.00  0.98  0.00  0.01  0.00  0.0\n",
      "2  0.00  0.00  0.98  0.00  0.01  0.0\n",
      "3  0.00  0.02  0.00  0.97  0.00  0.0\n",
      "4  0.00  0.00  0.01  0.01  0.98  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.985666666667\n",
      "\n",
      "overall confusion matrix :\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "      0     1     2     3     4    5\n",
      "0  0.99  0.00  0.01  0.00  0.00  0.0\n",
      "1  0.00  0.98  0.00  0.02  0.00  0.0\n",
      "2  0.00  0.00  0.99  0.00  0.01  0.0\n",
      "3  0.00  0.02  0.00  0.97  0.01  0.0\n",
      "4  0.00  0.00  0.01  0.01  0.98  0.0\n",
      "5  0.00  0.00  0.00  0.00  0.00  1.0\n",
      "\n",
      "\n",
      "accuracy:  0.985133333333\n",
      "garbage test1, predicted label :  [5]\n",
      "garbage test2, predicted label:  [5]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "driver program\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import config as cfg\n",
    "\n",
    "#loading input - output data and explore the dataset\n",
    "inputDataPath  = 'data/Proj2FeatVecsSet1.mat'\n",
    "outputDataPath = 'data/Proj2TargetOutputsSet1.mat'\n",
    "\n",
    "inputDataObj  = loadmat(inputDataPath)\n",
    "outputDataObj = loadmat(outputDataPath)\n",
    "\n",
    "inputData  = np.array(inputDataObj['Proj2FeatVecsSet1'])\n",
    "outputData = np.array(outputDataObj['Proj2TargetOutputsSet1'])\n",
    "\n",
    "# adding extra class to outputs\n",
    "outputData = np.array([np.append(output, [-1]) for output in outputData])\n",
    "\n",
    "# mixing in 5000 samples of noise\n",
    "X_garbage = np.reshape(np.random.rand(300000), (5000, 60))\n",
    "Y_garbage = np.array([np.array([-1,-1,-1,-1,-1, 1])]*5000)\n",
    "\n",
    "X = np.vstack((inputData, X_garbage))\n",
    "Y = np.vstack((outputData, Y_garbage))\n",
    "\n",
    "# noise added training data packed\n",
    "data = zipData(X, Y)\n",
    "\n",
    "# without noise training uncomment\n",
    "# data = zipData(inputData, outputData)\n",
    "\n",
    "# 5-fold cross-validation to obtain best model that prevents over-fitting\n",
    "Y_pred, EstParams, EstConfMatrices, ConfMatrix, best_idx = MyCrossValidate(data, cfg.k)\n",
    "\n",
    "# testing garbage class functionality\n",
    "garbage = [[5]*60]\n",
    "garbage2 = np.random.rand(1,60)\n",
    "\n",
    "Y_test = TestMyClassifier(zipData([X[0]], [1]), EstParams[best_idx])\n",
    "Y_test = TestMyClassifier(zipData([X[50]], [1]), EstParams[best_idx])\n",
    "\n",
    "print \"garbage test1, predicted label : \", TestMyClassifier(zipData(garbage, [1]), EstParams[best_idx])\n",
    "print \"garbage test2, predicted label: \", TestMyClassifier(zipData(garbage2, [1]), EstParams[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
