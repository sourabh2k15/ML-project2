{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input - output data and understand the structure\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "inputDataPath  = 'data/Proj2FeatVecsSet1.mat'\n",
    "outputDataPath = 'data/Proj2TargetOutputsSet1.mat'\n",
    "\n",
    "inputDataObj  = loadmat(inputDataPath)\n",
    "outputDataObj = loadmat(outputDataPath)\n",
    "\n",
    "inputData  = inputDataObj['Proj2FeatVecsSet1']\n",
    "outputData = outputDataObj['Proj2TargetOutputsSet1']\n",
    "\n",
    "data = zip(inputData, outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "computes confusion matrix\n",
    "\n",
    "@param   Y                   predicted labels\n",
    "\n",
    "@param   ClassLabels         actual / true labels  \n",
    "\n",
    "\"\"\" \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def MyConfusionMatrix(Y, ClassNames):\n",
    "    return confusion_matrix(Y, ClassNames)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "training script\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from skrvm import RVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def MyTrainClassifier(XEstimate, XValidate, Parameters):\n",
    "    \n",
    "    X_train, Y_train = zip(*XEstimate)\n",
    "        \n",
    "    X_train = np.array(list(X_train))\n",
    "    Y_train = np.array([np.where(output == 1)[0][0] for output in list(Y_train)])\n",
    "    \n",
    "    # sampling a small amount of training data for finding optimal hyper-parameters\n",
    "    X_hyper = X_train[:1000, :]\n",
    "    Y_hyper = Y_train[:1000]\n",
    "    \n",
    "    X_validate, Y_validate = zip(*XValidate)\n",
    "        \n",
    "    X_validate = np.array(list(X_validate))\n",
    "    Y_validate = np.array([np.where(output == 1)[0][0] for output in list(Y_validate)])\n",
    "    \n",
    "    # SVM\n",
    "    # all vs all pair training\n",
    "    if Parameters['algorithm'] == 'SVM':\n",
    "        \n",
    "        SVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate)\n",
    "    \n",
    "    elif Parameters['algorithm'] == 'RVM':\n",
    "        \n",
    "        RVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate)        \n",
    "    \n",
    "    elif Parameters['algorithm'] == 'Gaussian':\n",
    "     \n",
    "        Gaussian(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate):\n",
    "    \n",
    "    hyper_param_grid = [\n",
    "        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "        {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "    ]\n",
    "\n",
    "    estimator = GridSearchCV(SVC(decision_function_shape='ovo'), hyper_param_grid, cv=3, scoring='precision_macro')    \n",
    "    estimator.fit(X_hyper, Y_hyper) \n",
    "    \n",
    "    clf = estimator.best_estimator_\n",
    "    print \"found best estimator, training the estimator\"\n",
    "    \n",
    "    clf.fit(X_train[:4000, :], Y_train[:4000])\n",
    "    \n",
    "    print \"completed training\"\n",
    "    \n",
    "    print clf.score(X_validate, Y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RVM(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate):\n",
    "    clf = RVC(n_iter=1, kernel='linear')\n",
    "\n",
    "    start = time.clock()\n",
    "\n",
    "    clf.fit(X_hyper, Y_hyper)\n",
    "    print time.clock() - start, \"s\"\n",
    "\n",
    "    print clf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def Gaussian(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate):\n",
    "    print \"gaussian in progress\"\n",
    "    \n",
    "    kernel_rbf = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-5, 100000.0))\n",
    "    \n",
    "    clf = OneVsRestClassifier(GaussianProcessClassifier(kernel=kernel_rbf))\n",
    "    clf.fit(X_hyper, Y_hyper)\n",
    "    \n",
    "    print clf.predict_proba(X_validate[0])\n",
    "    \n",
    "    #myclf = OVO('gaussian')\n",
    "    #myclf.fit(X_train, Y_train)\n",
    "    \n",
    "    #print myclf.predict(X_validate[:500, :])\n",
    "    #print Y_validate[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    " K-fold cross validation script\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "from random import shuffle\n",
    "\n",
    "def MyCrossValidate(XTrain, Nf):\n",
    "    shuffle(XTrain)\n",
    "    kf = KFold(n_splits = Nf)\n",
    "    \n",
    "    j = 1\n",
    "    \n",
    "    for train_index, test_index in kf.split(XTrain):\n",
    "        En = [XTrain[i] for i in train_index]\n",
    "        Vn = [XTrain[i] for i in test_index]\n",
    "        \n",
    "        print \"fold {} in progress\".format(j)\n",
    "        \n",
    "        MyTrainClassifier(En, Vn, {'algorithm':'Gaussian'})\n",
    "        \n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVO:\n",
    "    def __init__(self, model):\n",
    "        self.model_ = model\n",
    "        \n",
    "    def fact(self, n):\n",
    "        if n == 0:\n",
    "            return 1\n",
    "        \n",
    "        return n*self.fact(n-1)\n",
    "\n",
    "    def nCr(self, n, r):\n",
    "        return self.fact(n)/(self.fact(n-r)*self.fact(r))\n",
    "    \n",
    "    def getModel(self):\n",
    "        if self.model_ == 'gaussian':\n",
    "            return GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.Nclasses_     = len(np.unique(Y))\n",
    "        self.Nclassifiers_ = self.nCr(self.Nclasses_, 2)\n",
    "        \n",
    "        Nclasses = self.Nclasses_\n",
    "        Nclassifiers = self.Nclassifiers_\n",
    "        \n",
    "        dataparts = [None]*Nclasses\n",
    "        classifiers = [[None]*Nclasses]*Nclasses\n",
    "        \n",
    "        print classifiers\n",
    "        \n",
    "        for i in range(Nclasses):\n",
    "            dataparts[i] = np.where(Y == i)[0]\n",
    "            \n",
    "        for i in range(Nclasses):\n",
    "            for j in range(i+1, Nclasses):\n",
    "                print \"training classifier: \", i, \" \",j\n",
    "                \n",
    "                xi = X[dataparts[i]]\n",
    "                xj = X[dataparts[j]]\n",
    "                \n",
    "                yi = [0]*len(xi)\n",
    "                yj = [1]*len(xj)\n",
    "                \n",
    "                x = np.vstack([xi, xj])\n",
    "                y = np.hstack([yi, yj])\n",
    "                \n",
    "                clf = self.getModel() \n",
    "                \n",
    "                print \"clf fitting\"\n",
    "                clf.fit(x, y)\n",
    "                print \"clf fitting done\"\n",
    "                \n",
    "                classifiers[i][j] = clf\n",
    "        \n",
    "        self.classifiers = classifiers\n",
    "        #print classifiers\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Nclasses = self.Nclasses_\n",
    "        Nclassifiers = self.Nclassifiers_\n",
    "        \n",
    "        classifiers = self.classifiers\n",
    "        \n",
    "        Y = []\n",
    "        \n",
    "        for x in X:\n",
    "            probabilities = [0]*Nclasses\n",
    "\n",
    "            for i in range(Nclasses):\n",
    "                for j in range(i+1, Nclasses):\n",
    "                    clf = classifiers[i][j]\n",
    "                \n",
    "                    probabilities[i] += clf.predict_proba(X)[0][0]\n",
    "                    probabilities[j] += clf.predict_proba(X)[0][1]\n",
    "                \n",
    "            Y.append(probabilities.index(max(probabilities)))\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 in progress\n",
      "gaussian in progress\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         0.         0.0703125  0.06510417\n 0.109375   0.046875   0.15364583 0.19270833 0.3046875  0.11458333\n 0.1953125  0.171875   0.28385417 0.1328125  0.11197917 0.04427083\n 0.08854167 0.06510417 0.04427083 0.07552083 0.1015625  0.03385417\n 0.234375   0.24479167 0.328125   0.1796875  0.296875   0.33854167\n 0.52083333 0.24479167 0.36197917 0.31770833 0.48177083 0.22395833\n 0.2578125  0.15364583 0.1953125  0.12760417 0.04427083 0.07552083\n 0.1015625  0.03385417 0.1640625  0.1796875  0.21875    0.1328125\n 0.14322917 0.14583333 0.21614583 0.13020833 0.16666667 0.14583333\n 0.19791667 0.09114583 0.14583333 0.109375   0.10677083 0.0625    ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3720d1702053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMyCrossValidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-7303aff199d3>\u001b[0m in \u001b[0;36mMyCrossValidate\u001b[0;34m(XTrain, Nf)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"fold {} in progress\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mMyTrainClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Gaussian'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-afd4572954f4>\u001b[0m in \u001b[0;36mMyTrainClassifier\u001b[0;34m(XEstimate, XValidate, Parameters)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mParameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Gaussian'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mGaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_hyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-e31f2d0cd4ef>\u001b[0m in \u001b[0;36mGaussian\u001b[0;34m(X_hyper, Y_hyper, X_train, Y_train, X_validate, Y_validate)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_hyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#myclf = OVO('gaussian')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vishi/anaconda3/envs/ml2/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vishi/anaconda3/envs/ml2/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Y[i, j] gives the probability that sample i has the label j.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# In the multi-label case, these are not disjoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vishi/anaconda3/envs/ml2/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    658\u001b[0m                              \u001b[0;34m\"predicting probability estimates. Use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                              \"one_vs_rest mode instead.\")\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vishi/anaconda3/envs/ml2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         0.         0.0703125  0.06510417\n 0.109375   0.046875   0.15364583 0.19270833 0.3046875  0.11458333\n 0.1953125  0.171875   0.28385417 0.1328125  0.11197917 0.04427083\n 0.08854167 0.06510417 0.04427083 0.07552083 0.1015625  0.03385417\n 0.234375   0.24479167 0.328125   0.1796875  0.296875   0.33854167\n 0.52083333 0.24479167 0.36197917 0.31770833 0.48177083 0.22395833\n 0.2578125  0.15364583 0.1953125  0.12760417 0.04427083 0.07552083\n 0.1015625  0.03385417 0.1640625  0.1796875  0.21875    0.1328125\n 0.14322917 0.14583333 0.21614583 0.13020833 0.16666667 0.14583333\n 0.19791667 0.09114583 0.14583333 0.109375   0.10677083 0.0625    ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "MyCrossValidate(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
